\relax 
\citation{hastie2009elements,lecun2015deep}
\citation{silver2016mastering}
\citation{chatgpt}
\citation{mcculloch1943logical}
\citation{rosenblatt1958perceptron}
\citation{rumelhart1985learning}
\citation{williams1989learning,sutskever2014sequence}
\citation{hochreiter1997long,chung2014empirical}
\citation{poole2014analyzing}
\citation{neelakantan2015adding}
\citation{luo2016understanding}
\citation{srivastava2014dropout}
\citation{szegedy2013intriguing}
\citation{goodfellow2014explaining}
\citation{carlini2017towards}
\citation{jiang2020beyond}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\citation{fonseca2003simulation}
\citation{lieu2022adaptive}
\citation{salle2014efficient}
\citation{liu2010stochastic}
\citation{gan2015valuation}
\citation{broadie2015risk}
\citation{hong2017kernel}
\citation{zhang2022sample}
\citation{rockafellar2002conditional}
\citation{eiopa2014underlying}
\citation{osfi2017life}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Formulation}{3}{}\protected@file@percent }
\newlabel{sec:problem-formulation}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Tail Risk Measures: VaR and CVaR}{3}{}\protected@file@percent }
\citation{geneva2013variable}
\citation{hardy2003investment}
\citation{dang2021efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Simulation Model for Variable Annuity Payouts}{4}{}\protected@file@percent }
\newlabel{subsec:VApayout}{{2.2}{4}}
\citation{cathcart2015calculating}
\citation{glasserman2004monte}
\newlabel{eq:delta}{{1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Dynamic Hedging for Variable Annuities}{5}{}\protected@file@percent }
\newlabel{subsec:dynamicHedge}{{2.3}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of multi-period nested simulation that estimates the P\&L for one outer scenario.\relax }}{5}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:illustration}{{1}{5}}
\citation{dang2020efficient}
\newlabel{eq:hedgingerror}{{2}{6}}
\newlabel{eq:lossrv}{{3}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Standard Nested Simulation Procedure for Estimating CVaR for GMWB Hedging Losses\relax }}{6}{}\protected@file@percent }
\newlabel{alg:standardProcedure}{{1}{6}}
\citation{dang2020efficient}
\@writefile{toc}{\contentsline {section}{\numberline {3}Two-Stage Nested Simulation with Metamodels}{7}{}\protected@file@percent }
\newlabel{sec:metamodel2Stage}{{3}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Two-Stage Metamodeling Nested Simulation Procedure for Estimating CVaR\relax }}{7}{}\protected@file@percent }
\newlabel{alg:twoStageProcedure}{{2}{7}}
\citation{dang2020efficient}
\citation{dang2020efficient}
\@writefile{toc}{\contentsline {section}{\numberline {4}Single-Stage Nested Simulation with Neural Network Metamodels}{8}{}\protected@file@percent }
\newlabel{sec:metamodel1Stage}{{4}{8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Single-Stage Metamodeling Nested Simulation Procedure for Estimating CVaR\relax }}{8}{}\protected@file@percent }
\newlabel{alg:oneStageProcedure}{{3}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Results}{8}{}\protected@file@percent }
\newlabel{sec:numerical}{{5}{8}}
\citation{kingma2014adam}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Architectures and MSEs of metamodels for GMWB inner simulation model.\relax }}{9}{}\protected@file@percent }
\newlabel{tab:gmwb_arch}{{1}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces QQ-plots between training labels (x-axis) and predicted losses (y-axis) for the RNN metamodel.\relax }}{10}{}\protected@file@percent }
\newlabel{fig:QQ_RNN}{{2}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces QQ-plots between true losses (x-axis) and predicted losses (y-axis) for regression metamodels.\relax }}{10}{}\protected@file@percent }
\newlabel{fig:QQ_REG}{{3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces QQ-plots between true losses (x-axis) and predicted losses (y-axis) for neural network metamodels.\relax }}{11}{}\protected@file@percent }
\newlabel{fig:QQ_NN}{{4}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Two-Stage Procedure}{11}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Percentage of correctly identified true tail scenarios by different proxy models.\relax }}{12}{}\protected@file@percent }
\newlabel{fig:tailMatches}{{5}{12}}
\newlabel{subfig:AllSafetyMargin}{{6a}{12}}
\newlabel{sub@subfig:AllSafetyMargin}{{a}{12}}
\newlabel{subfig:ZoomedSafetyMargin}{{6b}{12}}
\newlabel{sub@subfig:ZoomedSafetyMargin}{{b}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces 95\%-CVaR estimates by different procedures. Right figure is a zoomed-in version of left figure.\relax }}{12}{}\protected@file@percent }
\newlabel{fig:CVaR95}{{6}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Noise Tolerance of Deep Neural Network Metamodels}{12}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Architectures and MSEs of LSTM metamodels.\relax }}{13}{}\protected@file@percent }
\newlabel{tab:lstm_arch}{{2}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Single-Stage Procedure}{13}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces QQ-plots between true losses (x-axis) and predicted losses (y-axis) for two LSTM metamodels.\relax }}{14}{}\protected@file@percent }
\newlabel{fig:QQ_All}{{7}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{14}{}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{14}}
\bibstyle{plainnat}
\bibdata{refP2}
\bibcite{broadie2015risk}{{1}{2015}{{Broadie et~al.}}{{Broadie, Du, and Moallemi}}}
\bibcite{carlini2017towards}{{2}{2017}{{Carlini and Wagner}}{{}}}
\bibcite{cathcart2015calculating}{{3}{2015}{{Cathcart et~al.}}{{Cathcart, Lok, McNeil, and Morrison}}}
\bibcite{chung2014empirical}{{4}{2014}{{Chung et~al.}}{{Chung, Gulcehre, Cho, and Bengio}}}
\bibcite{dang2021efficient}{{5}{2021}{{Dang}}{{}}}
\bibcite{dang2020efficient}{{6}{2020}{{Dang et~al.}}{{Dang, Feng, and Hardy}}}
\bibcite{eiopa2014underlying}{{7}{2014}{{EIOPA}}{{}}}
\bibcite{fonseca2003simulation}{{8}{2003}{{Fonseca et~al.}}{{Fonseca, Navaresse, and Moynihan}}}
\bibcite{gan2015valuation}{{9}{2015}{{Gan and Lin}}{{}}}
\bibcite{glasserman2004monte}{{10}{2004}{{Glasserman}}{{}}}
\bibcite{goodfellow2014explaining}{{11}{2014}{{Goodfellow et~al.}}{{Goodfellow, Shlens, and Szegedy}}}
\bibcite{hardy2003investment}{{12}{2003}{{Hardy}}{{}}}
\bibcite{hastie2009elements}{{13}{2009}{{Hastie et~al.}}{{Hastie, Tibshirani, and Friedman}}}
\bibcite{hochreiter1997long}{{14}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{hong2017kernel}{{15}{2017}{{Hong et~al.}}{{Hong, Juneja, and Liu}}}
\bibcite{jiang2020beyond}{{16}{2020}{{Jiang et~al.}}{{Jiang, Huang, Liu, and Yang}}}
\bibcite{kingma2014adam}{{17}{2014}{{Kingma and Ba}}{{}}}
\bibcite{lecun2015deep}{{18}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{lieu2022adaptive}{{19}{2022}{{Lieu et~al.}}{{Lieu, Nguyen, Dang, Lee, Kang, and Lee}}}
\bibcite{liu2010stochastic}{{20}{2010}{{Liu and Staum}}{{}}}
\bibcite{luo2016understanding}{{21}{2016}{{Luo et~al.}}{{Luo, Li, Urtasun, and Zemel}}}
\bibcite{mcculloch1943logical}{{22}{1943}{{McCulloch and Pitts}}{{}}}
\bibcite{neelakantan2015adding}{{23}{2015}{{Neelakantan et~al.}}{{Neelakantan, Vilnis, Le, Sutskever, Kaiser, Kurach, and Martens}}}
\bibcite{chatgpt}{{24}{2023}{{OpenAI}}{{}}}
\bibcite{osfi2017life}{{25}{2017}{{OSFI}}{{}}}
\bibcite{poole2014analyzing}{{26}{2014}{{Poole et~al.}}{{Poole, Sohl-Dickstein, and Ganguli}}}
\bibcite{rockafellar2002conditional}{{27}{2002}{{Rockafellar and Uryasev}}{{}}}
\bibcite{rosenblatt1958perceptron}{{28}{1958}{{Rosenblatt}}{{}}}
\bibcite{rumelhart1985learning}{{29}{1985}{{Rumelhart et~al.}}{{Rumelhart, Hinton, and Williams}}}
\bibcite{salle2014efficient}{{30}{2014}{{Salle and Y{\i }ld{\i }zo{\u {g}}lu}}{{}}}
\bibcite{silver2016mastering}{{31}{2016}{{Silver et~al.}}{{Silver, Huang, Maddison, Guez, Sifre, Van Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, et~al.}}}
\bibcite{srivastava2014dropout}{{32}{2014}{{Srivastava et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{sutskever2014sequence}{{33}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{szegedy2013intriguing}{{34}{2013}{{Szegedy et~al.}}{{Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, and Fergus}}}
\bibcite{geneva2013variable}{{35}{2013}{{The Geneva Association}}{{}}}
\bibcite{williams1989learning}{{36}{1989}{{Williams and Zipser}}{{}}}
\bibcite{zhang2022sample}{{37}{2022}{{Zhang et~al.}}{{Zhang, Feng, Liu, and Wang}}}
\gdef \@abspage@last{17}
